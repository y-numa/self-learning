{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "        \n",
    "    # idxには抽出する行のインデックス（単語ID）を格納\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "    \n",
    "    #　重みの勾配をdwとして取り出す\n",
    "    # dW[...]=0でdWの要素を0で上書きする（dWを0にするのではなく、dWの形状を保ったまま、その要素を0にする）\n",
    "    # そして前走から伝わってくる勾配doutをidxで指定された行に代入する\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        \n",
    "        # 以下ではidxが重複したときに、どっちかの値が消えてしまう\n",
    "        dW[self.idx] = dout\n",
    "        \n",
    "        # 上記問題を解決した仕様が下記\n",
    "        # 勾配を加算していく\n",
    "        for i, word_id in enumerate(self.idx):\n",
    "            dW[word_id] += dout[i]\n",
    "        return None\n",
    "    \n",
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.forward(W)\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W*h, axis=1)\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0] , 1)\n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh\n",
    "    \n",
    "# Negative Samplingとは\n",
    "# 正例をターゲットとした場合の損失の算出と同時に、負例をいくつかサンプリングして同様に損失を算出する\n",
    "# そして。各データの損失を足し合わせて、最終的な損失とする\n",
    "# サンプリングの方法はコーパスから確率分布を作成し、その確率分布からサンプリングする\n",
    "# 具体的なフロー\n",
    "# コーパスから単語の確率分布を作成し、それを0.75乗する。\n",
    "# これらの処理をUnigramSamplerクラスを使って行う\n",
    "# corpus：単語IDのリスト、power：確率分布に対する累乗（デフォルトは0.75）、sample_size：負例サンプリングを行う個数\n",
    "class NegativeSamplingloss:\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layers = [SigmoidWithloss() for _ in range(sample_size + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
    "        self.params, self.grads = [], []\n",
    "        \n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample= self.sampler.get_negative_sample(target)\n",
    "        \n",
    "        # 正例のforward\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype = np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "        \n",
    "        # 負例のforward\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout = 1):\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "        return dh"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
